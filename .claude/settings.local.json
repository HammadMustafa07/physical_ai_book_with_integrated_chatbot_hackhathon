{
  "permissions": {
    "allow": [
      "Bash(git fetch --all --prune)",
      "Bash(.specify/scripts/powershell/create-new-feature.ps1 -Json \"Module 1: The Robotic Nervous System \\(ROS 2\\)\n\nAudience:\nAI and software students new to robotics.\n\nGoal:\nExplain how ROS 2 functions as the communication layer between AI logic and humanoid robot bodies.\n\nChapters \\(Docusaurus\\):\n\nChapter 1: What is ROS 2?\n- ROS 2 as a robotic nervous system\n- Distributed robot software model\n- Role in Physical AI and embodied intelligence\n\nChapter 2: ROS 2 Core Concepts\n- Nodes, Topics, Services, Actions\n- Pub/Sub communication model\n- Data flow between sensors and controllers\n\nChapter 3: From AI to Motion\n- Connecting Python AI agents via rclpy\n- Controllers and actuators overview\n- URDF basics for humanoid structure\" -Number 1 -ShortName \"ros2-nervous-system\")",
      "Bash(.specify/scripts/powershell/check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks)",
      "Bash(.specify/scripts/powershell/setup-plan.ps1 -Json)",
      "Bash(.specify/scripts/powershell/check-prerequisites.ps1 -Json)",
      "Bash(git rev-parse --git-dir)",
      "Bash(npx create-docusaurus@latest frontend_book classic --typescript false --package-manager npm)",
      "Bash(npx create-docusaurus@latest frontend_book classic)",
      "Bash(ls -la *.ts)",
      "Bash(ls -la side*)",
      "Bash(cat sidebars.ts)",
      "Bash(find . -name \"sidebars.ts\" -exec ls -la {} ;)",
      "Bash(.specify/scripts/powershell/create-new-feature.ps1 -Json \"Module 2: The Digital Twin \\(Gazebo & Unity\\)\n\nTarget audience:\n- Intermediate robotics & AI students\n- Developers who have completed Module 1 \\(ROS 2 fundamentals\\)\n- Learners transitioning from software-only AI to embodied simulation\n\nModule focus:\n- Physics-based simulation of humanoid robots\n- Creating accurate digital twins of real-world robots\n- Bridging ROS 2 with simulation engines for safe experimentation\n\nPrimary goal:\nEnable students to build, configure, and validate a humanoid robot digital twin capable of realistic physics, sensor feedback, and human–robot interaction before deployment to real hardware.\n\n\n\nContent structure \\(Docusaurus – 3 chapters\\):\n\nChapter 1: Digital Twins & Physics Simulation with Gazebo\n- Definition of a Digital Twin in robotics\n- Role of physics engines in Physical AI\n- Gazebo architecture and ROS 2 integration\n- Simulating gravity, inertia, friction, and collisions\n- Loading a humanoid URDF into Gazebo\n- Common simulation failure modes \\(explosions, joint instability\\)\n\nChapter 2: Sensor Simulation for Perception\n- Why sensor simulation matters before real hardware\n- Simulated LiDAR: range, noise, and scan topics\n- Simulated depth cameras and RGB-D pipelines\n- IMU simulation: orientation, acceleration, drift\n- Validating sensor data via ROS 2 topics\n- Synchronization issues between physics and sensor streams\n\nChapter 3: High-Fidelity Interaction with Unity\n- Why Unity is used alongside Gazebo\n- Gazebo vs Unity: physics accuracy vs visual realism\n- Human–robot interaction scenarios\n- Visual feedback, avatars, and interaction triggers\n- Conceptual Unity–ROS communication \\(no deep Unity coding\\)\n- Preparing simulations for later NVIDIA Isaac integration\" -Json -Number 1 -ShortName \"digital-twin\" \"Module 2: The Digital Twin \\(Gazebo & Unity\\)\")",
      "Bash(.specify/scripts/bash/create-new-feature.sh --json --number 1 --short-name \"digital-twin\" \"Module 2: The Digital Twin \\(Gazebo & Unity\\)\n\nTarget audience:\n- Intermediate robotics & AI students\n- Developers who have completed Module 1 \\(ROS 2 fundamentals\\)\n- Learners transitioning from software-only AI to embodied simulation\n\nModule focus:\n- Physics-based simulation of humanoid robots\n- Creating accurate digital twins of real-world robots\n- Bridging ROS 2 with simulation engines for safe experimentation\n\nPrimary goal:\nEnable students to build, configure, and validate a humanoid robot digital twin capable of realistic physics, sensor feedback, and human–robot interaction before deployment to real hardware.\n\n\n\nContent structure \\(Docusaurus – 3 chapters\\):\n\nChapter 1: Digital Twins & Physics Simulation with Gazebo\n- Definition of a Digital Twin in robotics\n- Role of physics engines in Physical AI\n- Gazebo architecture and ROS 2 integration\n- Simulating gravity, inertia, friction, and collisions\n- Loading a humanoid URDF into Gazebo\n- Common simulation failure modes \\(explosions, joint instability\\)\n\nChapter 2: Sensor Simulation for Perception\n- Why sensor simulation matters before real hardware\n- Simulated LiDAR: range, noise, and scan topics\n- Simulated depth cameras and RGB-D pipelines\n- IMU simulation: orientation, acceleration, drift\n- Validating sensor data via ROS 2 topics\n- Synchronization issues between physics and sensor streams\n\nChapter 3: High-Fidelity Interaction with Unity\n- Why Unity is used alongside Gazebo\n- Gazebo vs Unity: physics accuracy vs visual realism\n- Human–robot interaction scenarios\n- Visual feedback, avatars, and interaction triggers\n- Conceptual Unity–ROS communication \\(no deep Unity coding\\)\n- Preparing simulations for later NVIDIA Isaac integration\")",
      "Bash(mkdir -p specs/1-digital-twin)",
      "Bash(.specify/scripts/powershell/create-new-feature.ps1 -Json \"Module 4: Vision-Language-Action \\(VLA\\)\n\nTarget audience: Advanced robotics & AI students building autonomous humanoid systems  \nPlatform: Docusaurus \\(Markdown documentation\\)\n\nFocus:\nThe convergence of Large Language Models and Robotics through Vision-Language-Action pipelines.\nEnable humanoid robots to understand voice commands, reason cognitively, and execute physical actions via ROS 2.\n\nChapters to create \\(3 total\\):\n\nChapter 1: Voice-to-Action Interfaces  \n- Using OpenAI Whisper for real-time voice command recognition  \n- Mapping spoken commands to structured robot intents  \n- Integrating speech pipelines with ROS 2 nodes and topics  \n\nChapter 2: Cognitive Planning with LLMs  \n- Translating natural language goals \\(e.g., \"Clean the room\"\\) into multi-step action plans  \n- Task decomposition and sequencing using LLM reasoning  \n- Converting high-level plans into executable ROS 2 behaviors  \n\nChapter 3: Capstone – The Autonomous Humanoid  \n- End-to-end VLA pipeline: Voice → Plan → Navigate → Perceive → Manipulate  \n- Obstacle-aware navigation and object identification using computer vision  \n- Coordinating perception, planning, and manipulation in a simulated humanoid robot\" -Json -Number 2 -ShortName \"vla-module\")",
      "Bash(mkdir -p specs/2-vla-module)",
      "Bash(.specify/scripts/powershell/create-new-feature.ps1 -Json \"Module 4: Vision-Language-Action \\(VLA\\)\n\nTarget audience: Advanced robotics & AI students building autonomous humanoid systems  \nPlatform: Docusaurus \\(Markdown documentation\\)\n\nFocus:\nThe convergence of Large Language Models and Robotics through Vision-Language-Action pipelines.\nEnable humanoid robots to understand voice commands, reason cognitively, and execute physical actions via ROS 2.\n\nChapters to create \\(3 total\\):\n\nChapter 1: Voice-to-Action Interfaces  \n- Using OpenAI Whisper for real-time voice command recognition  \n- Mapping spoken commands to structured robot intents  \n- Integrating speech pipelines with ROS 2 nodes and topics  \n\nChapter 2: Cognitive Planning with LLMs  \n- Translating natural language goals \\(e.g., \"Clean the room\"\\) into multi-step action plans  \n- Task decomposition and sequencing using LLM reasoning  \n- Converting high-level plans into executable ROS 2 behaviors  \n\nChapter 3: Capstone – The Autonomous Humanoid  \n- End-to-end VLA pipeline: Voice → Plan → Navigate → Perceive → Manipulate  \n- Obstacle-aware navigation and object identification using computer vision  \n- Coordinating perception, planning, and manipulation in a simulated humanoid robot\" -Number 2 -ShortName \"vla-integration\")",
      "Bash(powershell -Command \".specify/scripts/powershell/create-new-feature.ps1 -Json \"\"Module 4: Vision-Language-Action \\(VLA\\)\n\nTarget audience: Advanced robotics & AI students building autonomous humanoid systems  \nPlatform: Docusaurus \\(Markdown documentation\\)\n\nFocus:\nThe convergence of Large Language Models and Robotics through Vision-Language-Action pipelines.\nEnable humanoid robots to understand voice commands, reason cognitively, and execute physical actions via ROS 2.\n\nChapters to create \\(3 total\\):\n\nChapter 1: Voice-to-Action Interfaces  \n- Using OpenAI Whisper for real-time voice command recognition  \n- Mapping spoken commands to structured robot intents  \n- Integrating speech pipelines with ROS 2 nodes and topics  \n\nChapter 2: Cognitive Planning with LLMs  \n- Translating natural language goals \\(e.g., \"Clean the room\"\\) into multi-step action plans  \n- Task decomposition and sequencing using LLM reasoning  \n- Converting high-level plans into executable ROS 2 behaviors  \n\nChapter 3: Capstone – The Autonomous Humanoid  \n- End-to-end VLA pipeline: Voice → Plan → Navigate → Perceive → Manipulate  \n- Obstacle-aware navigation and object identification using computer vision  \n- Coordinating perception, planning, and manipulation in a simulated humanoid robot\"\" -Number 2 -ShortName \"\"vla-integration\"\"\")",
      "Bash(mkdir -p specs/2-vla-integration)",
      "Bash(powershell -Command \".specify/scripts/powershell/setup-plan.ps1 -Json\")",
      "Bash(powershell -Command \".specify/scripts/powershell/update-agent-context.ps1 -AgentType claude\")",
      "Bash(find . -name \"*.md\" -exec grep -l \"Development Guidelines\\\\|Active Technologies\" {} ;)",
      "Bash(powershell -Command \".specify/scripts/powershell/check-prerequisites.ps1 -Json\")",
      "Bash(powershell -Command \".specify/scripts/powershell/check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks\")",
      "Bash(powershell -Command \"Get-Content specs/2-vla-integration/tasks.md | Measure-Object -Line\")",
      "Bash(mkdir -p specs/3-docusaurus-book-fix)",
      "Bash(powershell -Command \"if \\(Test-Path ''specs/3-docusaurus-book-fix/tasks.md''\\) { Get-Content ''specs/3-docusaurus-book-fix/tasks.md'' | Measure-Object -Line | Select-Object -ExpandProperty Lines } else { Write-Output ''File not found'' }\")",
      "Bash(npm install)",
      "Bash(npm run start)",
      "Bash(npm run build)",
      "Bash(powershell.exe -ExecutionPolicy Bypass -File \".specify/scripts/powershell/check-prerequisites.ps1\" -Json -RequireTasks -IncludeTasks)",
      "Bash(dir \"C:\\\\Users\\\\au603\\\\specs\\\\004-docusaurus-book-update\\\\tasks.md\")",
      "Bash(dir \"C:\\\\Users\\\\au603\\\\specs\\\\004-docusaurus-book-update\" /A)",
      "Skill(sp.implement)",
      "Bash(rm book_frontend/docs/modules/vla/intro.md)",
      "Bash(rm -rf book_frontend/.docusaurus)",
      "Bash(rm -rf book_frontend/build)",
      "Bash(npm run serve)",
      "Bash(cat \"C:\\\\Users\\\\au603\\\\Desktop\\\\Hackhathon\\\\physical-ai-hackhathon-book\\\\specs\\\\5-docusaurus-ui-fix\\\\spec.md\")",
      "Bash(powershell.exe -ExecutionPolicy Bypass -File \".specify/scripts/powershell/setup-plan.ps1\" -Json)",
      "Bash(powershell.exe -ExecutionPolicy Bypass -File \".specify/scripts/powershell/update-agent-context.ps1\" -AgentType claude)",
      "Bash(powershell.exe -ExecutionPolicy Bypass -File \".specify/scripts/powershell/check-prerequisites.ps1\" -Json)",
      "Bash(dir \"C:\\\\Users\\\\au603\\\\Desktop\\\\Hackhathon\\\\physical-ai-hackhathon-book\\\\specs\\\\5-docusaurus-ui-fix\\\\checklists\")",
      "Bash(timeout 60s npm run build)",
      "Bash(git checkout -b 001-homepage-theme-redesign)",
      "Bash(.specify/scripts/powershell/update-agent-context.ps1 -AgentType claude)",
      "Bash(node --version)",
      "Bash(npm --version)",
      "Bash(timeout 10s npm run start)",
      "Bash(git checkout -b 005-docusaurus-ui-fix)",
      "Bash(mkdir -p \"C:\\\\Users\\\\au603\\\\Desktop\\\\Hackhathon\\\\physical-ai-hackhathon-book\\\\specs\\\\005-docusaurus-ui-fix\\\\checklists\")",
      "Bash(mkdir -p \"C:\\\\Users\\\\au603\\\\Desktop\\\\Hackhathon\\\\physical-ai-hackhathon-book\\\\history\\\\prompts\\\\005-docusaurus-ui-fix\")",
      "Bash(cat)",
      "Bash(mkdir -p specs/006-dark-mode-ui-polish)",
      "Bash(powershell -ExecutionPolicy Bypass -File ./.specify/scripts/powershell/check-prerequisites.ps1 -Json)",
      "Bash(powershell -ExecutionPolicy Bypass -File ../../../.specify/scripts/powershell/check-prerequisites.ps1 -Json)",
      "Bash(powershell -ExecutionPolicy Bypass -File ./.specify/scripts/powershell/check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks)",
      "Bash(powershell -ExecutionPolicy Bypass -File \".specify/scripts/powershell/setup-plan.ps1\" -Json)",
      "Bash(powershell -ExecutionPolicy Bypass -File ./.specify/scripts/powershell/create-new-feature.ps1 -FeatureDescription \"Dark Mode UI Fix \\(Homepage Sections\\)\" -Number 9 -ShortName \"dark-mode-homepage-fix\" -Json)",
      "Skill(sp.plan)",
      "Bash(powershell -ExecutionPolicy Bypass -File ./.specify/scripts/powershell/setup-plan.ps1 -Json)",
      "Skill(sp.tasks)",
      "Bash(npm start)"
    ]
  }
}
